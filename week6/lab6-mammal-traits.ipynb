{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Random Forest Regression on Malagasy Mammal Traits\n",
    "\n",
    "### Introduction\n",
    " In Lab 5, we used functional trait data on bird species to predict their presence in a given climate type. This week we are using regression tree models on a similar dataset, this one on mammal species.  Our goal is to predict the total number of climate types each mammal species inhabits.\n",
    "\n",
    "You will compare performance of a single decision tree with a tuned random forest to get a sense of the effectiveness of ensemble methods with optimized hyperparameter values.\n",
    "\n",
    "There is a fair amount of missing data in this set, which can lead to low reliability of feature splits, increased overfitting, and decreased accuracy of tree models. You will address this issue with a combination of variable omission and imputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read data and retain a subset of the columns\n",
    "Similar to last week, we will need to take some steps to get the data in a usable form. We again reformat the diet-related variable names to match this set: `Diet_Vertebrates`,\n",
    "    `Diet_Fruits`, `Diet_Flowers`, `Diet_Seeds`, `Diet_Plants`, `Diet_Other`. Then drop columns that are not relevant functional traits or climate type variables.   Next, drop any remaining  variables that have greater than 40% of the observations missing. Print the final dataframe shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "dat = pd.read_csv(\"/courses/EDS232/Data/MamTraitData.csv\", na_values = \"-999\", encoding=\"ISO-8859-1\").rename(columns={\n",
    "    'Diet: invertebrates': 'Diet_Invertebrates',\n",
    "    'Diet: vertebrates': 'Diet_Vertebrates',\n",
    "    'Diet: fruits': 'Diet_Fruits',\n",
    "    'Diet: flower/nectar/pollen/gums': 'Diet_Flowers',\n",
    "    'Diet: seeds': 'Diet_Seeds',\n",
    "    'Diet: other plant materials': 'Diet_Plants',\n",
    "    'Diet: scavenge; garbage; carrion; carcasses': 'Diet_Other'\n",
    "})\n",
    "\n",
    "# View columns\n",
    "print(dat.columns)\n",
    "\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look at shape\n",
    "print(f\"Initial DataFrame shape: {dat.shape}\")\n",
    "\n",
    "# Look at na values\n",
    "print(f\"NA values:\\n {dat.isna().sum()}\")\n",
    "\n",
    "# Drop columns with na values over 40% \n",
    "df = dat.drop(['CranialCapacity', \n",
    "               'GestationLength', \n",
    "               'InterbirthInterval', \n",
    "               'HomeRange', \n",
    "               'PopulationDensity', \n",
    "               'SocialGrpSize', \n",
    "               'Longevity'], axis=1).copy()\n",
    "\n",
    "print(f\"Final DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Imputation with KNN\n",
    "Now impute the values of the missing data in the remaining numeric columns using k-nearest neighbors with `KNNImputer()`. Use the 5 nearest neighbors. This allows us to implement the knn algorithm to predict the missing values for an observation based on similar complete observations.Perform and print a check to ensure that there are no more NA values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separate numerical and categorical columns\n",
    "numerical = df.select_dtypes(exclude = ['object']).columns\n",
    "categorical = df.select_dtypes(include = ['object']).columns\n",
    "\n",
    "# Impute numerical columns\n",
    "imputer_num = KNNImputer(n_neighbors = 5)\n",
    "df[numerical] = imputer_num.fit_transform(df[numerical])\n",
    "\n",
    "# Impute categorical columns\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical] = imputer_cat.fit_transform(df[categorical])\n",
    "\n",
    "# Check\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create new column `nClimates`\n",
    "df['nClimates'] = df['Dry'] + df['Humid'] + df['Montane'] + df['Subarid'] + df['Subhumid']\n",
    "\n",
    "# Drop old columns\n",
    "df.drop(['Dry',\n",
    "         'Humid', \n",
    "         'Montane',\n",
    "         'Subarid',\n",
    "         'Subhumid'], axis=1, inplace = True)\n",
    "# Check\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recode categorical predictors to integers\n",
    "le = LabelEncoder()\n",
    "\n",
    "for column in categorical:\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    \n",
    "# Check    \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create target variable and numerical encoding \n",
    "Create the target variable (`nClimates`) representing the  number of climates in which a species was present. Then finish preparing the data to be used in our models.\n",
    "\n",
    "- Create a new column `nClimates` that combines the information of the five climate type variables\n",
    "- Drop the original `Dry`, `Humid`, `Montane`, `Subarid`, and `Subhumid` columns\n",
    "- Encode categorical variables\n",
    "- Split the dataset into training and test sets. Use a random state of 808. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "X = df[['AdultBodyMass',  \n",
    "        'Diet_Invertebrates', \n",
    "        'Diet_Vertebrates',\n",
    "        'Diet_Fruits', \n",
    "        'Diet_Flowers', \n",
    "        'Diet_Seeds', \n",
    "        'Diet_Plants', \n",
    "        'Diet_Other',\n",
    "       'HabitatBreadth',\n",
    "       'ActivityCycle',\n",
    "       'ForagingStratum',\n",
    "       'LitterSize']]\n",
    "\n",
    "y = df['nClimates']\n",
    "\n",
    "# Split the data into testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 808)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train a decision tree\n",
    "Train a single decision tree to predict the number of climate types inhabited by each species.\n",
    "\n",
    "- Train a `DecisionTreeRegressor`.  Let's apply a constraint on its growth by not allowing the tree to grow further than 5 levels. Use a random state of 808. \n",
    "- Make predictions\n",
    "- Visualize the decision tree using `plot_tree`\n",
    "- Evaluate and print its performance using mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single DT\n",
    "regressor = DecisionTreeRegressor(max_depth = 5, random_state = 808)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot decision tree\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_tree(regressor, filled=True, feature_names=df.columns, fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate mean squared error\n",
    "dt_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {dt_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the way this tree looks, what would you guess the most important variable to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would guess that the most important variable is `Diet_Plants` as it is the first split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Random forest with hyperparameter tuning\n",
    "Now we'll use a more sophisticated ensemble approach where we optimize the values of key hyperparameters that govern tree growth.\n",
    "\n",
    "- Define a parameter grid that includes the follow parameters and values:\n",
    "  -  (sqrt(p), 6, and no set #) of the number of features to try at each split\n",
    "  -  (50, 100, 200) total trees (learners)\n",
    "  -  a maximum tree depth of (3,4,5,6,7)\n",
    "  -  (2,5,10) minimum samples per split\n",
    "  -  (1,2,4) minimum samples per leaf\n",
    "  \n",
    "- Use `GridSearchCV` with 5-fold cross-validation and mse (`neg_mean_squared_error`) to find the best combination of parameter values\n",
    "- Train the best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200], # trees\n",
    "    \"max_depth\": [3, 4, 5, 6, 7], # levels of trees\n",
    "    \"max_features\": [\"sqrt\", 6, None], # m_try number of features tried at each partition\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initiate Random Forest\n",
    "rf = RandomForestRegressor(random_state = 808)\n",
    "\n",
    "# Find best parameters\n",
    "gs = GridSearchCV(rf, param_grid = param_grid, n_jobs = -1, return_train_score = True, scoring=\"neg_mean_squared_error\") # cv= 5 is default\n",
    "gs.fit(X_train, y_train)\n",
    "print(f\"Best params: {gs.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use best parameters\n",
    "best_rf = RandomForestRegressor(**gs.best_params_, random_state = 808)\n",
    "best_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Make predictions and evaluate random forest model\n",
    "Use the optimized random forest model (`best_rf`) to predict the number of climates a species inhabits and analyze its performance.\n",
    "\n",
    "- Use `best_rf` and `predict()` method on `X_test` to generate predictions\n",
    "- Print the best set of parameter values using thte `best_params` method\n",
    "- Compute and print the mse to evaluate model performance\n",
    "- Extract feature importance from the trained random forest model\n",
    "- Visualize feature importance using a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions from optimized random forest \n",
    "y_pred_best = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(f'Best parameters: {gs.best_params_}')\n",
    "print(f'MSE: {mean_squared_error(y_test, y_pred_best)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract feature importance\n",
    "feature_importance = best_rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_names = X_train.columns  # Assuming X_train is a Pandas DataFrame\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the feature importance\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=importance_df['Importance'], y=importance_df['Feature'], palette='Blues_r')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance in Optimized Random Forest Model\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate mse\n",
    "single = mean_squared_error(y_test, y_pred)\n",
    "ensemble = mean_squared_error(y_test, y_pred_best)\n",
    "\n",
    "print(f'Single Tree Accuracy: {single:.4f}')\n",
    "print(f'Ensemble Accuracy: {ensemble:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model had better accuracy, the single tree or the ensemble.  Interpret which traits have the most influence on species' climate adaptability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the single tree has better accuracy. The traits that have the most influence on species' climate adaptability include `AdultBodyMass`, `LitterSize`, and `Diet_Plants`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
